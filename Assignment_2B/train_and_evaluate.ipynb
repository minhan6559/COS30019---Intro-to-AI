{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79fd8901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting category-encoders\n",
      "  Downloading category_encoders-2.8.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: numpy>=1.14.0 in e:\\anaconda\\envs\\ita_tf\\lib\\site-packages (from category-encoders) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.0.5 in e:\\anaconda\\envs\\ita_tf\\lib\\site-packages (from category-encoders) (2.2.3)\n",
      "Collecting patsy>=0.5.1 (from category-encoders)\n",
      "  Downloading patsy-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.6.0 in e:\\anaconda\\envs\\ita_tf\\lib\\site-packages (from category-encoders) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in e:\\anaconda\\envs\\ita_tf\\lib\\site-packages (from category-encoders) (1.15.2)\n",
      "Collecting statsmodels>=0.9.0 (from category-encoders)\n",
      "  Downloading statsmodels-0.14.4-cp310-cp310-win_amd64.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\anaconda\\envs\\ita_tf\\lib\\site-packages (from pandas>=1.0.5->category-encoders) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\anaconda\\envs\\ita_tf\\lib\\site-packages (from pandas>=1.0.5->category-encoders) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\anaconda\\envs\\ita_tf\\lib\\site-packages (from pandas>=1.0.5->category-encoders) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in e:\\anaconda\\envs\\ita_tf\\lib\\site-packages (from scikit-learn>=1.6.0->category-encoders) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\anaconda\\envs\\ita_tf\\lib\\site-packages (from scikit-learn>=1.6.0->category-encoders) (3.6.0)\n",
      "Requirement already satisfied: packaging>=21.3 in e:\\anaconda\\envs\\ita_tf\\lib\\site-packages (from statsmodels>=0.9.0->category-encoders) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in e:\\anaconda\\envs\\ita_tf\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category-encoders) (1.17.0)\n",
      "Downloading category_encoders-2.8.1-py3-none-any.whl (85 kB)\n",
      "Downloading patsy-1.0.1-py2.py3-none-any.whl (232 kB)\n",
      "Downloading statsmodels-0.14.4-cp310-cp310-win_amd64.whl (9.8 MB)\n",
      "   ---------------------------------------- 0.0/9.8 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.0/9.8 MB 49.2 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.1/9.8 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.3/9.8 MB 11.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.3/9.8 MB 11.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.3/9.8 MB 11.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.3/9.8 MB 11.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.3/9.8 MB 4.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.3/9.8 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.8/9.8 MB 5.2 MB/s eta 0:00:00\n",
      "Installing collected packages: patsy, statsmodels, category-encoders\n",
      "Successfully installed category-encoders-2.8.1 patsy-1.0.1 statsmodels-0.14.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn==1.6.1 in e:\\anaconda\\envs\\ita_tf\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in e:\\anaconda\\envs\\ita_tf\\lib\\site-packages (from scikit-learn==1.6.1) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in e:\\anaconda\\envs\\ita_tf\\lib\\site-packages (from scikit-learn==1.6.1) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in e:\\anaconda\\envs\\ita_tf\\lib\\site-packages (from scikit-learn==1.6.1) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\anaconda\\envs\\ita_tf\\lib\\site-packages (from scikit-learn==1.6.1) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade category-encoders\n",
    "%pip install \"scikit-learn==1.6.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a891031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    ReduceLROnPlateau,\n",
    "    TensorBoard,\n",
    ")\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from src.train_and_evaluate.model_architecture import create_model\n",
    "from src.train_and_evaluate.load_data import load_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e5a298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34846600",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb46ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, model_name, fig_dir):\n",
    "    \"\"\"\n",
    "    Plot training history (loss and metrics)\n",
    "\n",
    "    Args:\n",
    "        history: Training history object\n",
    "        model_name: Name of the model\n",
    "        fig_dir: Directory to save plots\n",
    "    \"\"\"\n",
    "\n",
    "    # Plot loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.title(f\"{model_name} - Training and Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss (MSE)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(os.path.join(fig_dir, f\"{model_name}_loss.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot MAE\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history[\"mae\"], label=\"Training MAE\")\n",
    "    plt.plot(history.history[\"val_mae\"], label=\"Validation MAE\")\n",
    "    plt.title(f\"{model_name} - Training and Validation MAE\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Absolute Error\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(os.path.join(fig_dir, f\"{model_name}_mae.png\"), dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc044093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    X_train_inputs,\n",
    "    y_train,\n",
    "    X_val_inputs,\n",
    "    y_val,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    learning_rate=0.001,\n",
    "    clipnorm=1.0,\n",
    "    early_stopping_patience=10,\n",
    "    reduce_lr_patience=3,\n",
    "    verbose=1,\n",
    "    output_dir=\"checkpoints\",\n",
    "    model_name=\"traffic_model\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a model with early stopping, checkpoints, and learning rate schedules\n",
    "    \"\"\"\n",
    "    print(f\"Training model {model_name}...\")\n",
    "\n",
    "    # Format current datetime\n",
    "    current_time = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Create model, log and figure directories if they don't exist\n",
    "    model_save_dir = os.path.join(output_dir, \"saved_models\", model_name)\n",
    "    log_dir = os.path.join(output_dir, \"logs\", model_name)\n",
    "    fig_dir = os.path.join(output_dir, \"figures\", model_name)\n",
    "\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "    # Compile model with Adam optimizer and MSE loss\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate, clipnorm=clipnorm),\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"],\n",
    "    )\n",
    "\n",
    "    # Prepare callbacks\n",
    "    callbacks = []\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=early_stopping_patience,\n",
    "        verbose=verbose,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "    callbacks.append(early_stopping)\n",
    "\n",
    "    # Model checkpoint - update file extension from .h5 to .keras\n",
    "    model_path = os.path.join(model_save_dir, \"best.keras\")\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=model_path, monitor=\"val_loss\", verbose=verbose, save_best_only=True\n",
    "    )\n",
    "    callbacks.append(checkpoint)\n",
    "\n",
    "    # Learning rate reducer\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=reduce_lr_patience,  # Reduced from 5 to react faster\n",
    "        verbose=1,\n",
    "        min_lr=1e-6,\n",
    "    )\n",
    "    callbacks.append(reduce_lr)\n",
    "\n",
    "    # TensorBoard\n",
    "    tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True)\n",
    "    callbacks.append(tensorboard)\n",
    "    print(f\"TensorBoard logs saved to {log_dir}\")\n",
    "\n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train_inputs,\n",
    "        y_train,\n",
    "        validation_data=(X_val_inputs, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "    # Save final model - update file extension from .h5 to .keras\n",
    "    final_model_path = os.path.join(model_save_dir, f\"final_{model_name}.keras\")\n",
    "    model.save(final_model_path)\n",
    "\n",
    "    #Save final model architecture as JSON\n",
    "    model_json = model.to_json()\n",
    "    final_model_json_path = os.path.join(model_save_dir, f\"final_{model_name}.json\")\n",
    "    with open(final_model_json_path, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    # Save weights\n",
    "    weights = model.get_weights()\n",
    "    weights_path = os.path.join(model_save_dir, f\"final_{model_name}_weights.npz\")\n",
    "    np.savez(weights_path, *weights)\n",
    "\n",
    "    print(f\"Model saved to {model_save_dir} folder\")\n",
    "\n",
    "    # Plot training history\n",
    "    plot_training_history(history, model_name, fig_dir)\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"history\": history.history,\n",
    "        \"training_time\": training_time,\n",
    "        \"model_path\": model_save_dir,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723c046a",
   "metadata": {},
   "source": [
    "### Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b547711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_actual_vs_predicted(y_true, y_pred, model_name, output_dir):\n",
    "    \"\"\"\n",
    "    Create scatter plot of actual vs predicted values\n",
    "\n",
    "    Args:\n",
    "        y_true: True values\n",
    "        y_pred: Predicted values\n",
    "        model_name: Name of the model\n",
    "        output_dir: Directory to save the plot\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "\n",
    "    # Add perfect prediction line\n",
    "    max_val = max(np.max(y_true), np.max(y_pred))\n",
    "    plt.plot([0, max_val], [0, max_val], \"r--\")\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Actual Traffic Volume\")\n",
    "    plt.ylabel(\"Predicted Traffic Volume\")\n",
    "    plt.title(f\"{model_name} - Actual vs Predicted Traffic Volume\")\n",
    "\n",
    "    # Add metrics text\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    metrics_text = (\n",
    "        f\"MSE: {mse:.2f}\\n\" f\"RMSE: {rmse:.2f}\\n\" f\"MAE: {mae:.2f}\\n\" f\"R²: {r2:.2f}\"\n",
    "    )\n",
    "    plt.annotate(\n",
    "        metrics_text,\n",
    "        xy=(0.05, 0.95),\n",
    "        xycoords=\"axes fraction\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", alpha=0.8),\n",
    "    )\n",
    "\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{model_name}_scatter.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Also plot histogram of errors\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(y_true - y_pred, bins=50, alpha=0.75)\n",
    "    plt.axvline(x=0, color=\"r\", linestyle=\"--\")\n",
    "    plt.xlabel(\"Error (Actual - Predicted)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"{model_name} - Prediction Error Distribution\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{model_name}_error_hist.png\"), dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1da13aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_examples(\n",
    "    results, model_name, output_dir, max_points=288, num_locations=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot examples of predictions for specific locations with time values on x-axis\n",
    "\n",
    "    Args:\n",
    "        results: Dictionary with actual and predicted values\n",
    "        model_name: Name of the model\n",
    "        output_dir: Directory to save the plots\n",
    "        max_points: Maximum number of timepoints to visualize (default: 288)\n",
    "        num_locations: Number of locations to sample (default: 10)\n",
    "    \"\"\"\n",
    "    # Get unique locations\n",
    "    locations = np.unique(results[\"location\"])\n",
    "\n",
    "    # Select up to num_locations locations\n",
    "    sample_locations = locations[: min(num_locations, len(locations))]\n",
    "\n",
    "    for location in sample_locations:\n",
    "        # Filter data for this location\n",
    "        mask = results[\"location\"] == location\n",
    "\n",
    "        # Convert to arrays to avoid indexing issues\n",
    "        dates = np.array(results[\"target_date\"][mask])\n",
    "        times = np.array(results[\"target_time\"][mask])\n",
    "        actual = np.array(results[\"actual\"][mask])\n",
    "        predicted = np.array(results[\"predicted\"][mask])\n",
    "\n",
    "        # Create a timestamp by combining date and time\n",
    "        timestamps = [f\"{d} {t}\" for d, t in zip(dates, times)]\n",
    "\n",
    "        # Sort by timestamp\n",
    "        sort_idx = np.argsort(timestamps)\n",
    "        timestamps = [timestamps[i] for i in sort_idx]\n",
    "        actual = actual[sort_idx]\n",
    "        predicted = predicted[sort_idx]\n",
    "        sorted_times = [times[i] for i in sort_idx]  # Now this should work\n",
    "\n",
    "        # Take only max_points to avoid overcrowding\n",
    "        if len(timestamps) > max_points:\n",
    "            timestamps = timestamps[:max_points]\n",
    "            actual = actual[:max_points]\n",
    "            predicted = predicted[:max_points]\n",
    "            sorted_times = sorted_times[:max_points]\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(actual, \"b-\", label=\"Actual\", linewidth=1.5)\n",
    "        plt.plot(predicted, \"r-\", label=\"Predicted\", linewidth=1.5)\n",
    "\n",
    "        # Create x-axis labels with time values\n",
    "        if len(sorted_times) > 200:\n",
    "            tick_spacing = len(sorted_times) // 20  # Show ~20 ticks\n",
    "        elif len(sorted_times) > 100:\n",
    "            tick_spacing = len(sorted_times) // 15  # Show ~15 ticks\n",
    "        else:\n",
    "            tick_spacing = max(1, len(sorted_times) // 10)  # Show ~10 ticks\n",
    "\n",
    "        # Set x-ticks at regular intervals\n",
    "        tick_positions = range(0, len(sorted_times), tick_spacing)\n",
    "        tick_labels = [sorted_times[i] for i in tick_positions]\n",
    "\n",
    "        plt.xticks(tick_positions, tick_labels, rotation=45)\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.title(\n",
    "            f\"{model_name} - Predictions for {location}\\n(showing {len(timestamps)} points)\"\n",
    "        )\n",
    "        plt.xlabel(\"Time of Day\")\n",
    "        plt.ylabel(\"Traffic Volume\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Create safe filename\n",
    "        safe_location = location.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "        plt.savefig(\n",
    "            os.path.join(\n",
    "                output_dir,\n",
    "                f\"{model_name}_{safe_location}_{max_points}points_example.png\",\n",
    "            ),\n",
    "            dpi=300,\n",
    "        )\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7528e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model,\n",
    "    X_test_inputs,\n",
    "    y_test,\n",
    "    meta_test,\n",
    "    model_name,\n",
    "    output_dir=\"evaluations\",\n",
    "    max_visualization_points=288,\n",
    "    num_visualization_locations=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on test data and generate evaluation metrics and plots\n",
    "\n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        X_test_inputs: Test inputs (list of arrays for feature and location inputs)\n",
    "        y_test: Test targets\n",
    "        meta_test: Test metadata\n",
    "        model_name: Model name for saving results\n",
    "        output_dir: Directory to save evaluation results\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    print(f\"Evaluating model {model_name} on test data...\")\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Generate predictions\n",
    "    y_pred = model.predict(X_test_inputs)\n",
    "    y_pred = y_pred.flatten()  # Ensure 1D array\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "    # Create results DataFrame\n",
    "    results = {\n",
    "        \"actual\": y_test,\n",
    "        \"predicted\": y_pred,\n",
    "        \"error\": y_test - y_pred,\n",
    "        \"location\": meta_test[\"Location\"],\n",
    "        \"target_date\": meta_test[\"target_date\"],\n",
    "        \"target_time\": meta_test[\"target_time\"],\n",
    "    }\n",
    "\n",
    "    # Create plots\n",
    "    # Plot actual vs predicted\n",
    "    plot_actual_vs_predicted(y_test, y_pred, model_name, output_dir)\n",
    "\n",
    "    # Plot prediction examples for a few locations\n",
    "    plot_prediction_examples(\n",
    "        results,\n",
    "        model_name,\n",
    "        output_dir,\n",
    "        max_points=max_visualization_points,\n",
    "        num_locations=num_visualization_locations,\n",
    "    )\n",
    "\n",
    "    return {\"mse\": mse, \"rmse\": rmse, \"mae\": mae, \"r2\": r2, \"predictions\": y_pred}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817d0b5b",
   "metadata": {},
   "source": [
    "### Most important part. Set the configurations for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43407296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data from processed_data/preprocessed_data/...\n",
      "Loaded all processed data successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load processed data\n",
    "from src.train_and_evaluate.model_architecture import create_gru_model\n",
    "\n",
    "data = load_processed_data(input_dir=\"processed_data/preprocessed_data/\")\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Define models to train\n",
    "models_to_train = [\n",
    "    {\n",
    "        \"type\": \"gru\",\n",
    "        \"create_model_func\": create_gru_model,\n",
    "        \"model_params\": {\"gru_units\": 64, \"dropout_rate\": 0.2},\n",
    "        \"train_params\": {\"epochs\": 50, \"batch_size\": 64},\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cdfc67",
   "metadata": {},
   "source": [
    "### Code to train and evaluate all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e204df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training gru_20250505_110110...\n",
      "==================================================\n",
      "Creating gru model...\n",
      "WARNING:tensorflow:Layer gru_layer will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " location_input (InputLayer)    [(None, 24)]         0           []                               \n",
      "                                                                                                  \n",
      " feature_input (InputLayer)     [(None, 24, 12)]     0           []                               \n",
      "                                                                                                  \n",
      " location_embedding (Embedding)  (None, 24, 16)      2192        ['location_input[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 24, 28)       0           ['feature_input[0][0]',          \n",
      "                                                                  'location_embedding[0][0]']     \n",
      "                                                                                                  \n",
      " gru_layer (GRU)                (None, 64)           18048       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1)            65          ['gru_layer[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 20,305\n",
      "Trainable params: 20,305\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Training model gru_20250505_110110...\n",
      "TensorBoard logs saved to checkpoints\\logs\\gru_20250505_110110\n",
      "Epoch 1/50\n",
      "  54/4810 [..............................] - ETA: 13:05 - loss: 17980.3340 - mae: 102.4535"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m training_results \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[0;32m     45\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     46\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39mcheckpoint_dir,\n\u001b[0;32m     47\u001b[0m     X_train_inputs\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_train_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     48\u001b[0m     y_train\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_train\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     49\u001b[0m     X_val_inputs\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_test_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     50\u001b[0m     y_val\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_test\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     51\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrain_params,\n\u001b[0;32m     53\u001b[0m )\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[0;32m     56\u001b[0m evaluation_results \u001b[38;5;241m=\u001b[39m evaluate_model(\n\u001b[0;32m     57\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     58\u001b[0m     X_test_inputs\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_test_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluations\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_name),\n\u001b[0;32m     63\u001b[0m )\n",
      "Cell \u001b[1;32mIn[5], line 77\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, X_train_inputs, y_train, X_val_inputs, y_val, epochs, batch_size, learning_rate, clipnorm, early_stopping_patience, reduce_lr_patience, verbose, output_dir, model_name)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m     76\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 77\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\ita_tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\ita_tf\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\ita_tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\ita_tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\ita_tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\ita_tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\ita_tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\ita_tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\ita_tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train each model\n",
    "results = {}\n",
    "for model_config in models_to_train:\n",
    "    # Set default parameters if None\n",
    "    model_params = model_config.get(\"model_params\", {})\n",
    "    train_params = model_config.get(\"train_params\", {})\n",
    "    model_type = model_config[\"type\"]\n",
    "\n",
    "    current_time = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_name = f\"{model_type}_{current_time}\"\n",
    "    model_config[\"name\"] = model_name\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {model_config['name']}...\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    # Get required parameters from data\n",
    "    seq_length = data[\"X_train\"].shape[1]\n",
    "    n_features = data[\"n_features\"]\n",
    "    n_locations = data[\"n_locations\"]\n",
    "\n",
    "    # Create model\n",
    "    model_params.update(\n",
    "        {\n",
    "            \"seq_length\": seq_length,\n",
    "            \"n_features\": n_features,\n",
    "            \"n_locations\": n_locations,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"Creating {model_type} model...\")\n",
    "\n",
    "    if (\n",
    "        \"create_model_func\" in model_config\n",
    "    ):  # Check if a custom model creation function is provided\n",
    "        create_model_func = model_config[\"create_model_func\"]\n",
    "        model = create_model_func(**model_params)\n",
    "    else:  # Use default create_model function\n",
    "        model = create_model(model_type=model_type, **model_params)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # Train model\n",
    "    training_results = train_model(\n",
    "        model=model,\n",
    "        output_dir=checkpoint_dir,\n",
    "        X_train_inputs=data[\"X_train_inputs\"],\n",
    "        y_train=data[\"y_train\"],\n",
    "        X_val_inputs=data[\"X_test_inputs\"],\n",
    "        y_val=data[\"y_test\"],\n",
    "        model_name=model_name,\n",
    "        **train_params,\n",
    "    )\n",
    "\n",
    "    # Evaluate model\n",
    "    evaluation_results = evaluate_model(\n",
    "        model=model,\n",
    "        X_test_inputs=data[\"X_test_inputs\"],\n",
    "        y_test=data[\"y_test\"],\n",
    "        meta_test=data[\"meta_test\"],\n",
    "        model_name=model_name,\n",
    "        output_dir=os.path.join(checkpoint_dir, \"evaluations\", model_name),\n",
    "    )\n",
    "\n",
    "    results[model_config[\"name\"]] = {\n",
    "        \"model\": model,\n",
    "        \"training_results\": training_results,\n",
    "        \"evaluation_results\": evaluation_results,\n",
    "    }\n",
    "\n",
    "# Print summary of results\n",
    "print(\"\\n\\nTraining and Evaluation Summary:\")\n",
    "print(f\"{'='*50}\")\n",
    "for name, result in results.items():\n",
    "    eval_result = result[\"evaluation_results\"]\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  RMSE: {eval_result['rmse']:.4f}\")\n",
    "    print(f\"  MAE: {eval_result['mae']:.4f}\")\n",
    "    print(f\"  R²: {eval_result['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21ab0da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ita_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
