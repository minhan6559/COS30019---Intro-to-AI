{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6a37ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f654bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_preprocessing.clean_data import process_data\n",
    "from src.data_preprocessing.feature_engineering import run_feature_engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1cf9c7",
   "metadata": {},
   "source": [
    "### Convert to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebf2621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New file path for the uploaded .xlsx file\n",
    "xlsx_path = \"raw_data/modified_scats_data_oct_2006.xlsx\"\n",
    "\n",
    "# Attempt to read the .xlsx file using openpyxl engine\n",
    "excel_data = pd.read_excel(xlsx_path, engine=\"openpyxl\")\n",
    "\n",
    "# Save it as a CSV file\n",
    "csv_path = \"raw_data/modified_scats_data_oct_2006.xlsx\"\n",
    "# excel_data.to_csv(csv_path, index=False)\n",
    "\n",
    "# Read the CSV file back\n",
    "csv_data = pd.read_csv(csv_path)\n",
    "print(csv_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bcbc71",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e565d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_data(\n",
    "    file_path=\"raw_data/modified_scats_data_oct_2006.csv\",\n",
    "    output_dir=\"processed_data/preprocessed_data\",\n",
    "    visualize_output_dir=\"processed_data/eda_insights\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0139e3af",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6478da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date column to datetime\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"]).dt.normalize()\n",
    "\n",
    "output_dir = \"processed_data/preprocessed_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "seq_length = 24\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Run feature engineering\n",
    "processed_data = run_feature_engineering(\n",
    "    df,\n",
    "    seq_length=seq_length,\n",
    "    test_ratio=test_ratio,\n",
    "    output_dir=output_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f2db92",
   "metadata": {},
   "source": [
    "### Process traffic metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8219f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_roads(location):\n",
    "    \"\"\"Extract road names from location string.\"\"\"\n",
    "    if not isinstance(location, str):\n",
    "        return []\n",
    "\n",
    "    # Extract base road names (without directional qualifiers)\n",
    "    # First, find common patterns for road names like \"WARRIGAL_RD\" or \"HIGH STREET_RD\"\n",
    "    road_patterns = re.findall(\n",
    "        r\"([A-Z]+(?:[ _][A-Z]+)*(?:[ _](?:RD|ST|HWY|FWY|GV|ARTERIAL|RAMPS|PARK))?)\",\n",
    "        location,\n",
    "    )\n",
    "\n",
    "    # Clean up road names\n",
    "    cleaned_roads = []\n",
    "    for road in road_patterns:\n",
    "        # Skip directional prefixes if they're standalone\n",
    "        if road in [\"N\", \"S\", \"E\", \"W\", \"NE\", \"NW\", \"SE\", \"SW\", \"OF\"]:\n",
    "            continue\n",
    "\n",
    "        # Skip if the road name contains directional information that would indicate\n",
    "        # it's a full location rather than just a road name\n",
    "        if re.search(r\" OF | N OF | S OF | E OF | W OF |NW OF|NE OF|SW OF|SE OF\", road):\n",
    "            continue\n",
    "\n",
    "        cleaned_roads.append(road)\n",
    "\n",
    "    return cleaned_roads\n",
    "\n",
    "\n",
    "def process_traffic_metadata(\n",
    "    filepath, output_dir, \n",
    "    locations_to_keep=set(),\n",
    "    scats_numbers_to_keep=set(),\n",
    "):\n",
    "    \"\"\"Process traffic data to create site metadata JSON file.\"\"\"\n",
    "    print(\"=== Data Processing ===\")\n",
    "\n",
    "    # 1. Load raw data\n",
    "    excel_file = filepath\n",
    "\n",
    "    try:\n",
    "        traffic = pd.read_excel(excel_file, sheet_name=\"Data\", header=1)\n",
    "        summary = pd.read_excel(excel_file, sheet_name=\"Summary Of Data\", header=3)\n",
    "        print(f\"Successfully loaded Excel file: {excel_file}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find Excel file at {excel_file}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Excel file: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Process summary data to group locations by site ID\n",
    "    site_locations = {}\n",
    "    current_site_id = None\n",
    "\n",
    "    # Cast all SCATS numbers in scats_numbers_to_keep to string for comparison\n",
    "    scats_numbers_to_keep = {str(int(num)) for num in scats_numbers_to_keep}\n",
    "\n",
    "    # Cast all locations in locations_to_keep to string for comparison\n",
    "    locations_to_keep = {str(loc) for loc in locations_to_keep}\n",
    "\n",
    "    # Iterate through rows in summary sheet\n",
    "    for _, row in summary.iterrows():\n",
    "        # If the row has a SCATS Number, update the current site ID\n",
    "        if not pd.isna(row[\"SCATS Number\"]):\n",
    "            current_site_id = str(int(row[\"SCATS Number\"]))\n",
    "\n",
    "            # Initialize the site ID in the dictionary if not already present\n",
    "            # Also Check if the site ID is in the list of SCATS numbers to keep\n",
    "            if current_site_id not in site_locations and current_site_id in scats_numbers_to_keep:\n",
    "                site_locations[current_site_id] = []\n",
    "\n",
    "        # If we have a current site ID and the row has a Location\n",
    "        if current_site_id and not pd.isna(row[\"Location\"]):\n",
    "            location = row[\"Location\"]\n",
    "\n",
    "            # Check if the location is in the list of locations to keep\n",
    "            if isinstance(location, str) and location in locations_to_keep:\n",
    "                site_locations[current_site_id].append(location)\n",
    "\n",
    "    print(f\"Found {len(site_locations)} unique SCATS sites\")\n",
    "\n",
    "    # 3. Process metadata for each site\n",
    "    site_metadata = {}\n",
    "\n",
    "    for site_id, locations in site_locations.items():\n",
    "        # Extract all connected roads\n",
    "        all_roads = set()\n",
    "        for loc in locations:\n",
    "            # Alternative approach to extract just road names\n",
    "            parts = re.split(\n",
    "                r\"[ _]OF[ _]|[ _]of[ _]|[ _]N[ _]of[ _]|[ _]S[ _]of[ _]|[ _]E[ _]of[ _]|[ _]W[ _]of[ _]|NW OF|NE OF|SW OF|SE OF\",\n",
    "                loc,\n",
    "            )\n",
    "            for part in parts:\n",
    "                if part and len(part) > 1:  # Skip empty parts or single letters\n",
    "                    # Extract just the road name (removing directional indicators)\n",
    "                    road_match = re.match(\n",
    "                        r\"([A-Z]+(?:[ _][A-Z]+)*(?:[ _](?:RD|ST|HWY|FWY|GV|ARTERIAL|RAMPS|PARK)))\",\n",
    "                        part.strip(),\n",
    "                    )\n",
    "                    if road_match:\n",
    "                        road_name = road_match.group(1).strip()\n",
    "                        if road_name and road_name not in [\n",
    "                            \"N\",\n",
    "                            \"S\",\n",
    "                            \"E\",\n",
    "                            \"W\",\n",
    "                            \"NE\",\n",
    "                            \"NW\",\n",
    "                            \"SE\",\n",
    "                            \"SW\",\n",
    "                            \"OF\",\n",
    "                        ]:\n",
    "                            all_roads.add(road_name)\n",
    "\n",
    "        # Get lat/lon from traffic data\n",
    "        site_traffic = traffic[traffic[\"SCATS Number\"] == int(site_id)]\n",
    "\n",
    "        if (\n",
    "            not site_traffic.empty\n",
    "            and \"NB_LATITUDE\" in site_traffic.columns\n",
    "            and \"NB_LONGITUDE\" in site_traffic.columns\n",
    "        ):\n",
    "            lat = float(site_traffic[\"NB_LATITUDE\"].iloc[0])\n",
    "            lon = float(site_traffic[\"NB_LONGITUDE\"].iloc[0])\n",
    "        else:\n",
    "            lat = None\n",
    "            lon = None\n",
    "\n",
    "        # Create metadata entry\n",
    "        site_metadata[site_id] = {\n",
    "            \"site_id\": int(site_id),\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon,\n",
    "            \"locations\": locations,  # All locations for this site\n",
    "            \"connected_roads\": list(all_roads),\n",
    "        }\n",
    "\n",
    "        print(\n",
    "            f\"→ Processed Site {site_id}: {len(locations)} locations, {len(all_roads)} connected roads\"\n",
    "        )\n",
    "\n",
    "    metadata_path = os.path.join(output_dir, \"sites_metadata.json\")\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(site_metadata, f, indent=2)\n",
    "\n",
    "    print(\n",
    "        f\"\\nMetadata processing complete! Created {metadata_path} with {len(site_metadata)} sites.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "614498ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Processing ===\n",
      "Successfully loaded Excel file: raw_data/original_scats_data_oct_2006.xlsx\n",
      "Found 39 unique SCATS sites\n",
      "→ Processed Site 970: 4 locations, 2 connected roads\n",
      "→ Processed Site 2000: 4 locations, 3 connected roads\n",
      "→ Processed Site 2200: 4 locations, 2 connected roads\n",
      "→ Processed Site 2820: 2 locations, 3 connected roads\n",
      "→ Processed Site 2825: 1 locations, 2 connected roads\n",
      "→ Processed Site 2827: 4 locations, 4 connected roads\n",
      "→ Processed Site 2846: 4 locations, 2 connected roads\n",
      "→ Processed Site 3001: 3 locations, 3 connected roads\n",
      "→ Processed Site 3002: 4 locations, 3 connected roads\n",
      "→ Processed Site 3120: 4 locations, 3 connected roads\n",
      "→ Processed Site 3122: 3 locations, 2 connected roads\n",
      "→ Processed Site 3126: 3 locations, 2 connected roads\n",
      "→ Processed Site 3127: 3 locations, 2 connected roads\n",
      "→ Processed Site 3180: 3 locations, 2 connected roads\n",
      "→ Processed Site 3662: 4 locations, 4 connected roads\n",
      "→ Processed Site 3682: 4 locations, 2 connected roads\n",
      "→ Processed Site 3685: 3 locations, 2 connected roads\n",
      "→ Processed Site 3804: 4 locations, 2 connected roads\n",
      "→ Processed Site 3812: 4 locations, 3 connected roads\n",
      "→ Processed Site 4030: 3 locations, 4 connected roads\n",
      "→ Processed Site 4032: 4 locations, 3 connected roads\n",
      "→ Processed Site 4034: 4 locations, 3 connected roads\n",
      "→ Processed Site 4035: 4 locations, 3 connected roads\n",
      "→ Processed Site 4040: 6 locations, 3 connected roads\n",
      "→ Processed Site 4043: 4 locations, 2 connected roads\n",
      "→ Processed Site 4051: 3 locations, 3 connected roads\n",
      "→ Processed Site 4057: 4 locations, 2 connected roads\n",
      "→ Processed Site 4063: 4 locations, 2 connected roads\n",
      "→ Processed Site 4262: 1 locations, 2 connected roads\n",
      "→ Processed Site 4263: 4 locations, 2 connected roads\n",
      "→ Processed Site 4264: 4 locations, 2 connected roads\n",
      "→ Processed Site 4266: 4 locations, 2 connected roads\n",
      "→ Processed Site 4270: 4 locations, 2 connected roads\n",
      "→ Processed Site 4272: 3 locations, 2 connected roads\n",
      "→ Processed Site 4273: 2 locations, 2 connected roads\n",
      "→ Processed Site 4321: 4 locations, 4 connected roads\n",
      "→ Processed Site 4324: 3 locations, 2 connected roads\n",
      "→ Processed Site 4812: 3 locations, 2 connected roads\n",
      "→ Processed Site 4821: 4 locations, 3 connected roads\n",
      "\n",
      "Metadata processing complete! Created processed_data/preprocessed_data\\sites_metadata.json with 39 sites.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\ita_tf\\lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    }
   ],
   "source": [
    "file_path = \"raw_data/original_scats_data_oct_2006.xlsx\"\n",
    "output_dir = \"processed_data/preprocessed_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "cleaned_df = pd.read_csv(\"processed_data\\preprocessed_data\\cleaned_data.csv\")\n",
    "locations_to_keep = set(cleaned_df[\"Location\"].unique())\n",
    "scats_num_to_keep = set(cleaned_df[\"SCATS Number\"].unique())\n",
    "\n",
    "process_traffic_metadata(\n",
    "    filepath=file_path, output_dir=output_dir, \n",
    "    locations_to_keep=locations_to_keep,\n",
    "    scats_numbers_to_keep=scats_num_to_keep,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d107cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ita_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
